{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a038c8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e4f2ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PreferedAlbum</th>\n",
       "      <th>Vinyl</th>\n",
       "      <th>AvgTime</th>\n",
       "      <th>TicketInfo</th>\n",
       "      <th>Opinion</th>\n",
       "      <th>Concert</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>Food</th>\n",
       "      <th>Drinks</th>\n",
       "      <th>FreePass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/2362</td>\n",
       "      <td>Head Hunters</td>\n",
       "      <td>False</td>\n",
       "      <td>3.055</td>\n",
       "      <td>B/140/N</td>\n",
       "      <td>Everything worked, yet nothing surprised me. (...</td>\n",
       "      <td>NYC12-5pm</td>\n",
       "      <td>22.0</td>\n",
       "      <td>False</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04/0599</td>\n",
       "      <td>Bright Size Life</td>\n",
       "      <td>True</td>\n",
       "      <td>2.510</td>\n",
       "      <td>LB/124/N</td>\n",
       "      <td>Solid gig—nothing magical, nothing awful. (NYC...</td>\n",
       "      <td>NYC12-5pm</td>\n",
       "      <td>9.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/3223</td>\n",
       "      <td>Head Hunters</td>\n",
       "      <td>False</td>\n",
       "      <td>0.803</td>\n",
       "      <td>CB/523/N</td>\n",
       "      <td>Guitars were sharp, but everything else felt r...</td>\n",
       "      <td>NYC12-5pm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>815.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/1966</td>\n",
       "      <td>Kind of Blue</td>\n",
       "      <td>False</td>\n",
       "      <td>0.568</td>\n",
       "      <td>A/81/XL</td>\n",
       "      <td>Feedback screamed louder than the guitars. Dis...</td>\n",
       "      <td>FL16-7pm</td>\n",
       "      <td>36.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1427.0</td>\n",
       "      <td>1395.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/8850</td>\n",
       "      <td>Bright Size Life</td>\n",
       "      <td>False</td>\n",
       "      <td>0.034</td>\n",
       "      <td>B/580/XL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NYC12-5pm</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id     PreferedAlbum  Vinyl  AvgTime TicketInfo  \\\n",
       "0  01/2362      Head Hunters  False    3.055    B/140/N   \n",
       "1  04/0599  Bright Size Life   True    2.510   LB/124/N   \n",
       "2  01/3223      Head Hunters  False    0.803   CB/523/N   \n",
       "3  01/1966      Kind of Blue  False    0.568    A/81/XL   \n",
       "4  01/8850  Bright Size Life  False    0.034   B/580/XL   \n",
       "\n",
       "                                             Opinion    Concert   Age    VIP  \\\n",
       "0  Everything worked, yet nothing surprised me. (...  NYC12-5pm  22.0  False   \n",
       "1  Solid gig—nothing magical, nothing awful. (NYC...  NYC12-5pm   9.0  False   \n",
       "2  Guitars were sharp, but everything else felt r...  NYC12-5pm   NaN  False   \n",
       "3  Feedback screamed louder than the guitars. Dis...   FL16-7pm  36.0  False   \n",
       "4                                                NaN  NYC12-5pm  16.0  False   \n",
       "\n",
       "     Food  Drinks  FreePass  \n",
       "0   170.0     1.0      True  \n",
       "1     0.0     0.0      True  \n",
       "2   815.0    37.0     False  \n",
       "3  1427.0  1395.0     False  \n",
       "4  1299.0     0.0     False  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('complex-master-learn-and-get-your-mark/train.csv')\n",
    "df_test = pd.read_csv('complex-master-learn-and-get-your-mark/test.csv')\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f34f65c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c481d631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='FreePass', ylabel='AvgTime'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqUElEQVR4nO3dD1iV9f3/8Tf4B/wH6kxQRLMQxT+oYCW2b2pp/stpba25Naw0V+ufstlGLgsJseXfpfm/zJrTtGm7SiuzoTNsiX+aWtpMg1qAuitBSECB3/X+/C4YR4EgOdznfHg+ruu+OPd97nPuzzle55yXn8/7vj8+paWlpQIAAGAJX6cbAAAAUJcINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAVmksDUxJSYl8/fXX0qpVK/Hx8XG6OQAAoAb0snznzp2Tjh07iq9v9X0zDS7caLAJDQ11uhkAAOB7+PLLL6VTp07V7tPgwo322JS9OQEBAU43BwAA1EBubq7pnCj7Ha9Ogws3ZUNRGmwINwAAeJealJRQUAwAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAVvGYcDNnzhxzSeWpU6dWu9/GjRulR48e4u/vL3369JGtW7fWWxsBAIDn84hws3fvXlm+fLlERkZWu19qaqpMmDBBJk2aJAcOHJDx48eb5fDhw/XWVgAA4NkcDzd5eXnyi1/8QlauXClt2rSpdt9FixbJyJEjZfr06RIRESGJiYkSFRUlixcvrrf2AgAAz+b4rOAPPfSQjBkzRoYNGybPPPNMtfvu2bNH4uLiXLaNGDFCtmzZUuVjCgsLzVJxynQAaOgKCgokIyPD6Waggs6dO5uSC3h5uFm/fr3s37/fDEvVRFZWlgQFBbls03XdXpXk5GRJSEi44rYCgE002EyZMsXpZqCCFStWSHh4OO+JN4ebL7/8Uh577DHZvn27W5NqfHy8S2+P9tyEhoa67XgA4C29BPpj6s3S09MlKSlJZsyYIV26dBEb/k3g5eFm3759curUKVMzU6a4uFh27dplamh0KKlRo0YujwkODpbs7GyXbbqu26vi5+dnFgDA/+h/Km3pJdBgY8trgZcXFN9yyy1y6NAhOXjwYPkyYMAAU1ysty8NNiomJkZ27Njhsk17fnQ7AACAoz03rVq1kt69e7tsa9GihfzgBz8o3x4bGyshISGmbkbpMNbgwYNl3rx5pghZa3bS0tK8vmsVAABYdCr4dxW8ZWZmlq8PGjRI1q1bZ8JM3759ZdOmTeZMqUtDEgAAaLgcPxW8opSUlGrX1Z133mkWAAAAr+u5AQAAqC3CDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVRwNN0uXLpXIyEgJCAgwS0xMjGzbtq3K/desWSM+Pj4ui7+/f722GQAAeLbGTh68U6dOMmfOHOnWrZuUlpbKyy+/LOPGjZMDBw5Ir169Kn2MhqBjx46Vr2vAAQAA8IhwM3bsWJf1pKQk05vz4YcfVhluNMwEBwfXUwsBAIC3cTTcVFRcXCwbN26U/Px8MzxVlby8POnSpYuUlJRIVFSUzJ49u8ogpAoLC81SJjc3t87bDqBhyc7OlpycHKeb0eClp6e7/IWzAgMDJSgoyCP+GXxKdTzIQYcOHTJhpqCgQFq2bCnr1q2T0aNHV7rvnj175N///rep09Evlrlz58quXbvkyJEjZoirMk8//bQkJCRctl0fr0NcAFDbYHP3L2PlQtH//tMEQKRJUz959ZW1bgs42jmhAaomv9+Oh5uioiLJyMgwjd20aZOsWrVKdu7cKT179vzOx164cEEiIiJkwoQJkpiYWOOem9DQUMINgO/ls88+kylTpsj5awZLiX8g7yIgIr4FOdLsxE5ZsWKFhIeHOx5uHB+Watq0qYSFhZnb0dHRsnfvXlm0aJEsX778Ox/bpEkT6d+/vxw/frzKffz8/MwCAHVJg01Ji3a8qYAH8rjr3GgtTcWelu+q09FhrQ4dOri9XQAAwDs42nMTHx8vo0aNks6dO8u5c+dMvU1KSoq888475v7Y2FgJCQmR5ORksz5r1iwZOHCg6ek5e/asPPfcc6aQbPLkyU6+DAAA4EEcDTenTp0yASYzM9OMo2mhsAab4cOHm/u1FsfX93+dS998843cf//9kpWVJW3atDHDWKmpqTWqzwEAAA2Do+Fm9erV1d6vvTgVLViwwCwAAABeU3MDAABwJQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWcTTcLF26VCIjIyUgIMAsMTExsm3btmofs3HjRunRo4f4+/tLnz59ZOvWrfXWXgAA4PkcDTedOnWSOXPmyL59+yQtLU1uvvlmGTdunBw5cqTS/VNTU2XChAkyadIkOXDggIwfP94shw8frve2AwAAz+RouBk7dqyMHj1aunXrJuHh4ZKUlCQtW7aUDz/8sNL9Fy1aJCNHjpTp06dLRESEJCYmSlRUlCxevLje2w4AADxTY/EQxcXFZsgpPz/fDE9VZs+ePRIXF+eybcSIEbJly5Yqn7ewsNAsZXJzc+uw1QAaKt/zZ51uAuAxfD3s8+B4uDl06JAJMwUFBabXZvPmzdKzZ89K983KypKgoCCXbbqu26uSnJwsCQkJdd5uAA1bs5O7nG4CAE8NN927d5eDBw9KTk6ObNq0SSZOnCg7d+6sMuDUVnx8vEtvj/bchIaG1slzA2i4zne9SUqatXa6GYDH9Nw086DA73i4adq0qYSFhZnb0dHRsnfvXlNbs3z58sv2DQ4OluzsbJdtuq7bq+Ln52cWAKhLGmxKWrTjTQU8kMdd56akpMSlRqYiHb7asWOHy7bt27dXWaMDAAAaHkd7bnTIaNSoUdK5c2c5d+6crFu3TlJSUuSdd94x98fGxkpISIipm1GPPfaYDB48WObNmydjxoyR9evXm1PIV6xY4eTLAAAAHsTRcHPq1CkTYDIzMyUwMNBc0E+DzfDhw839GRkZ4uv7v86lQYMGmQD0hz/8QZ544glzCrmeKdW7d28HXwUAAPAkjoab1atXV3u/9uJc6s477zQLAACAV9TcAAAAXAnCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVRwNN8nJyXLddddJq1atpH379jJ+/Hg5duxYtY9Zs2aN+Pj4uCz+/v711mYAAODZHA03O3fulIceekg+/PBD2b59u1y4cEFuvfVWyc/Pr/ZxAQEBkpmZWb6kp6fXW5sBAIBna+zkwd9+++3LemW0B2ffvn1y0003Vfk47a0JDg6uhxYCAABv42i4uVROTo7527Zt22r3y8vLky5dukhJSYlERUXJ7NmzpVevXpXuW1hYaJYyubm5ddxqAA2Rb8H//74CIB73efCYcKNBZerUqXLjjTdK7969q9yve/fu8uKLL0pkZKQJQ3PnzpVBgwbJkSNHpFOnTpXW9SQkJLi59QAaisDAQGnS1E/kxE6nmwJ4lCZN/cznwxP4lJaWlooHePDBB2Xbtm2ye/fuSkNKVbROJyIiQiZMmCCJiYk16rkJDQ01wUhrdwCgtrKzs8t7muEcrbdMSkqSGTNmmN58OCswMFCCgoLc9vz6+63HqMnvt0f03Dz88MPy5ptvyq5du2oVbFSTJk2kf//+cvz48Urv9/PzMwsA1BX9AnfnlzhqR4NNeHg4bxs842wp7TTSYLN582Z5//33pWvXrrV+juLiYjl06JB06NDBLW0EAADexdGeGz0NfN26dfLGG2+Ya91kZWWZ7drt1KxZM3M7NjZWQkJCTO2MmjVrlgwcOFDCwsLk7Nmz8txzz5muycmTJzv5UgAAgIdwNNwsXbrU/B0yZIjL9pdeeknuuececzsjI0N8ff/XwfTNN9/I/fffb4JQmzZtJDo6WlJTU6Vnz5713HoAAOCJHA03NallTklJcVlfsGCBWQAAACrD3FIAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFW+d7gpKiqSY8eOycWLF+u2RQAAAPUZbr799luZNGmSNG/eXHr16mWmR1CPPPKIzJkz50raAgAAUP/hJj4+Xj7++GMzLYK/v3/59mHDhsmGDRuuvEUAAAD1ObfUli1bTIjRmbl9fHzKt2svzueff34lbQEAAKj/npvTp09L+/btL9uen5/vEnYAAAC8ItwMGDBA3nrrrfL1skCzatUqiYmJqdvWAQAAuHtYavbs2TJq1Cj55JNPzJlSixYtMrdTU1Nl586dtX06AAAAZ3tufvjDH8rBgwdNsOnTp4+8++67Zphqz549Eh0dXbetAwAAcHfPjbr22mtl5cqV3+ehAAAAnhdu1KlTp8xSUlLisj0yMrIu2gUAAFA/4Wbfvn0yceJE+fTTT6W0tNTlPi0uLi4u/n4tAQAAcCLc3HfffRIeHi6rV6+WoKAgTv8GAADeHW5OnDghr7/+uoSFhbmnRQAAAPV5ttQtt9xipl8AAACwoudGL9anNTeHDx+W3r17S5MmTVzu/9GPflSX7QMAAHBvuNHr2XzwwQeybdu2y+6joBgAAHjdsNQjjzwid999t2RmZprTwCsunCkFAAC8Ltz897//lWnTppkzpQAAALw+3Nxxxx3y97//3T2tAQAAqO+aG73GTXx8vOzevdvMLXVpQfGjjz56pW0CAACo37OlWrZsaWYAv3QWcC0oJtwAAACvCjcnT550T0sAAACcqLkBAADw+p6buLg4SUxMlBYtWpjb1Zk/f36ND56cnCx//etf5ejRo9KsWTMZNGiQPPvss9K9e/dqH7dx40Z58skn5YsvvpBu3bqZx4wePbrGxwUAAA083Bw4cEAuXLhQfrsqWnNTG1qz89BDD8l1110nFy9elCeeeEJuvfVW+eSTT0yQqkxqaqpMmDDBBKPbbrtN1q1bJ+PHj5f9+/ebKyYDAICGzae0tLS0JjuuXbtW7rrrLvHz83NbY06fPi3t27c3oeemm26qdB9tQ35+vrz55pvl2wYOHCj9+vWTZcuWfecxcnNzJTAwUHJyciQgIKBO2w8AqD+fffaZTJkyRVasWGHO5IXdcmvx+13jguJ7771XRo4cacKHu2iDVdu2baud/uHSobERI0bIli1bKt2/sLDQLBXfHABo6AoKCiQjI0O8WXp6ustfb9e5c2fx9/d3uhlWqHG4qWEHz/em0zdMnTpVbrzxxmqHl7Kysi67OrKu6/bK6PBVQkJCnbcXALyZBhvt9bBBUlKS2IAeKIdOBa9tTU1taO2NzjSuFwesS3rBwYo9PdpzExoaWqfHAABv7CXQH1N41r8JHAg3t9xyizRuXP1DtLC3th5++GFTQ7Nr1y7p1KlTtfsGBwdLdna2yzZd1+2V0Rohd9YJAYA30uEP6lRgq1qFG61t0asT1xUd6tJZxjdv3iwpKSnStWvX73xMTEyM7Nixwwxhldm+fbvZDgAAUKtwM3369DotKNahKD2V+4033pBWrVqV181oNbRe90bFxsZKSEiIqZ1Rjz32mAwePFjmzZsnY8aMkfXr10taWhrdqwAAoHZXKHZHvc3SpUvNGVJDhgyRDh06lC8bNmxwKXrLzMwsX9cL/Wkg0rHivn37yqZNm8yZUlzjBgAA1Oo6N76+vqZnxZ2ngtcHrnMDAIDdv9++tZkw86qrrqqL9gEAADhfc9OlSxfzt6q5pXTYSqvvw8LCZNy4cdVeiA8AAMDxYakyQ4cONad7FxcXl09wqZfAbtSokfTo0UOOHTtmgo5er6Znz57iaRiWAgDA+7hlWKqM9soMGzZMvv76a9m3b59ZvvrqKxk+fLiZ0PI///mPmRdq2rRpV/IaAAAA6qfnRk/L1uvKXNorc+TIETOjt4Yb7dnR22fOnBFPQ88NAADex609N/qkp06dqnRG77JJKVu3bi1FRUW1fWoAAIAr9r2Gpe677z5zVWEdjtJFb0+aNEnGjx9v9vnoo4+4rDcAAPCOYam8vDxTT7N27Vq5ePGi2abzTU2cOFEWLFggLVq0kIMHD5rt/fr1E0/DsBQAAN6nNr/ftQ43FUPOiRMnzO1rrrmmTueccifCDQAA3setNTevvvqqfPvttybMREZGmsVbgg0AALBfrcONDknpFAw///nPZevWreZ6NwAAAF4bbnQSS52JWy/U99Of/tRMdKmze6emprqnhQAAALXwvWtulA5P6ZlSOkv3e++9J506dZLPP/9cPBk1NwAAeJ/a/H7XeG6pyjRv3lxGjBgh33zzjaSnp8unn356JU8HAABQ/8NSZT02f/7zn2X06NHmisULFy6U22+/3VylGAAAwKvCzc9+9jNTUKyFxXoKeEpKihw/flwSExPLr3sDAADglFoPS+ns36+99poZjtLb586dkxUrVsjq1aslLS2Ns6cAAIB3hRsdjlK7du0ygeb111+Xjh07yh133CGLFy92RxsBAADcE26ysrJkzZo1JtRo1bKeCl5YWChbtmy5bJZwAAAAj665GTt2rHTv3l0+/vhjU0D89ddfy/PPP+/e1gEAALir52bbtm3y6KOPyoMPPijdunWr7XEAAAA8q+dm9+7dpng4OjpabrjhBlNfc+bMGfe2DgAAwF3hZuDAgbJy5Uoz/cKvfvUrMwWDFhKXlJTI9u3bTfABAADw6ukXjh07ZoqLX3nlFTl79qwMHz5c/va3v4knY/oFAAC8T21+v7/XFYrLaIHxH//4R/nqq6/kL3/5y5U8FQAAgPM9N96InhsAALxPvfXcAAAAeBrCDQAAsArhBgAAWIVwAwAArEK4AQAAVnE03OjM4jpnlV4M0MfHx0zAWZ2UlBSz36WLTugJAADgeLjJz8+Xvn37ypIlS2p98UC9UnLZ0r59e7e1EQAAWDpxpjuMGjXKLLWlYaZ169ZuaRMAAPBuXllz069fP+nQoYOZ7uGDDz6odt/CwkJz4Z+KCwAAsJdXhRsNNMuWLZPXX3/dLKGhoTJkyBDZv39/lY9JTk42VzQsW/QxAADAXh4z/YIWBm/evFnGjx9fq8cNHjxYOnfubCbvrKrnRpcy2nOjAacml28GAADeN/2CozU3deH666+X3bt3V3m/n5+fWQAAQMPgVcNSlTl48KAZrgIAAHC85yYvL0+OHz9evn7y5EkTVtq2bWuGmuLj4+U///mPrF271ty/cOFC6dq1q/Tq1UsKCgpk1apV8v7778u7777r4KsAAACexNFwk5aWJkOHDi1fj4uLM38nTpwoa9asMdewycjIKL+/qKhIfvOb35jA07x5c4mMjJT33nvP5TkAAEDD5jEFxZ5YkAQAALzv99vra24AAAAqItwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFjF0XCza9cuGTt2rHTs2FF8fHxky5Yt3/mYlJQUiYqKEj8/PwkLC5M1a9bUS1sBAIB3cDTc5OfnS9++fWXJkiU12v/kyZMyZswYGTp0qBw8eFCmTp0qkydPlnfeecftbQUAAN6hsZMHHzVqlFlqatmyZdK1a1eZN2+eWY+IiJDdu3fLggULZMSIEW5sKQAA8BaOhpva2rNnjwwbNsxlm4Ya7cGpSmFhoVnK5ObmurWNNikoKJCMjAynm4EKOnfuLP7+/rwnAGBLuMnKypKgoCCXbbqugeX8+fPSrFmzyx6TnJwsCQkJ9dhKe2iwmTJlitPNQAUrVqyQ8PBw3hMAsCXcfB/x8fESFxdXvq5BKDQ01NE2eVMvgf6YerP09HRJSkqSGTNmSJcuXcSGfxMAgEXhJjg4WLKzs1226XpAQEClvTZKz6rSBbWnwx+29BJosLHltQAALLrOTUxMjOzYscNl2/bt2812AAAAx8NNXl6eOaVbl7JTvfV2WRGrDinFxsaW7//AAw/IiRMn5PHHH5ejR4/KCy+8IK+99ppMmzbNsdcAAAA8i6PhJi0tTfr3728WpbUxenvmzJlmPTMz0+VsHT0N/K233jK9NXp9HD0lfNWqVZwGDgAAPKPmZsiQIVJaWlrl/ZVdfVgfc+DAATe3DAAAeCuvqrkBAAD4LoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKzi6KzgNsvOzpacnBynm9Hgpaenu/yFswIDAyUoKIh/BgBu5VNaWloqDUhubq75gtXgERAQ4LZgc/cvY+VCUaFbnh/wVk2a+smrr6wl4ABw6+83PTduoG+8Bpvz1wyWEv9AdxwC8Dq+BTkiJ3aazwe9NwDciXDjRhpsSlq0c+chAADAJSgoBgAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALCKR4SbJUuWyNVXXy3+/v5yww03yEcffVTlvmvWrBEfHx+XRR8HAADgEeFmw4YNEhcXJ0899ZTs379f+vbtKyNGjJBTp05V+ZiAgADJzMwsX9LT0+u1zQAAwHM5Hm7mz58v999/v9x7773Ss2dPWbZsmTRv3lxefPHFKh+jvTXBwcHlS1BQUL22GQAAeK7GTh68qKhI9u3bJ/Hx8eXbfH19ZdiwYbJnz54qH5eXlyddunSRkpISiYqKktmzZ0uvXr0q3bewsNAsZXJzc6W++J4/W2/HAjwdnwcADSLcnDlzRoqLiy/redH1o0ePVvqY7t27m16dyMhIycnJkblz58qgQYPkyJEj0qlTp8v2T05OloSEBHFCs5O7HDkuAAANmaPh5vuIiYkxSxkNNhEREbJ8+XJJTEy8bH/tFdKanoo9N6GhofXS1vNdb5KSZq3r5ViAN/TcEPgBWB9u2rVrJ40aNZLs7GyX7bqutTQ10aRJE+nfv78cP3680vv9/PzM4gQNNiUt2jlybAAAGipHC4qbNm0q0dHRsmPHjvJtWkej6xV7Z6qjw1qHDh2SDh06uLGlAADAWzg+LKVDRhMnTpQBAwbI9ddfLwsXLpT8/Hxz9pSKjY2VkJAQUzujZs2aJQMHDpSwsDA5e/asPPfcc+ZU8MmTJzv8SgAAgCdwPNzcddddcvr0aZk5c6ZkZWVJv3795O233y4vMs7IyDBnUJX55ptvzKnjum+bNm1Mz09qaqo5jRwAAMDxcKMefvhhs1QmJSXFZX3BggVmAQAA8MiL+AEAANQlwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArOIRE2fayrcgx+kmAB6DzwOA+kK4cYPAwEBp0tRP5MROdzw94LX0c6GfDwBwJ8KNGwQFBcmrr6yVnBx6bpyWnp4uSUlJMmPGDOnSpYvTzWnwNNjo5wMA3Ilw4yb6Bc6XuOfQYBMeHu50MwAA9YCCYgAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVPCLcLFmyRK6++mrx9/eXG264QT766KNq99+4caP06NHD7N+nTx/ZunVrvbUVAAB4NsfDzYYNGyQuLk6eeuop2b9/v/Tt21dGjBghp06dqnT/1NRUmTBhgkyaNEkOHDgg48ePN8vhw4frve0AAMDzOB5u5s+fL/fff7/ce++90rNnT1m2bJk0b95cXnzxxUr3X7RokYwcOVKmT58uERERkpiYKFFRUbJ48eJ6bzsAAPA8jZ08eFFRkezbt0/i4+PLt/n6+sqwYcNkz549lT5Gt2tPT0Xa07Nly5ZK9y8sLDRLmdzc3Dprv+0KCgokIyNDvFl6errLX2/XuXNnMxwLAPDQcHPmzBkpLi6WoKAgl+26fvTo0Uofk5WVVen+ur0yycnJkpCQUIetbjg02EyZMkVskJSUJDZYsWKFhIeHO90MAPBojoab+qC9QhV7erTnJjQ01NE2eVMvgf6YwrP+TQAAHhxu2rVrJ40aNZLs7GyX7boeHBxc6WN0e2329/PzMwtqT4c/6CUAAHgbRwuKmzZtKtHR0bJjx47ybSUlJWY9Jiam0sfo9or7q+3bt1e5PwAAaFgcH5bSIaOJEyfKgAED5Prrr5eFCxdKfn6+OXtKxcbGSkhIiKmdUY899pgMHjxY5s2bJ2PGjJH169dLWloawycAAMAzws1dd90lp0+flpkzZ5qi4H79+snbb79dXjSsRa16BlWZQYMGybp16+QPf/iDPPHEE9KtWzdzplTv3r0dfBUAAMBT+JSWlpZKA6IFxYGBgZKTkyMBAQFONwcAANTx77fjF/EDAACoS4QbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqjk+/UN/KLsisVzoEAADeoex3uyYTKzS4cHPu3DnzNzQ01OmmAACA7/E7rtMwVKfBzS1VUlIiX3/9tbRq1Up8fHycbg7qIelrkP3yyy+ZSwywDJ/vhqW0tNQEm44dO7pMqF2ZBtdzo29Ip06dnG4G6plOssZEqYCd+Hw3HIHf0WNThoJiAABgFcINAACwCuEGVvPz85OnnnrK/AVgFz7fqEqDKygGAAB2o+cGAABYhXADAACsQrgBAABWIdzAWmvWrJHWrVs73QwAQD0j3MDj3XPPPeZq0pcux48fd7ppAOpIZZ/xisvTTz/Ne40aa3BXKIZ3GjlypLz00ksu26666irH2gOgbmVmZpbf3rBhg8ycOVOOHTtWvq1ly5blt/Uk3+LiYmncmJ8wVI6eG3jN9SyCg4NdlkWLFkmfPn2kRYsWZv6oX//615KXl1flc3z88ccydOhQM6+YXq49Ojpa0tLSyu/fvXu3/N///Z80a9bMPN+jjz4q+fn59fQKgYat4mdbL7GvvTVl60ePHjWf223btpnPrX4f6OdVe3XHjx/v8jxTp06VIUOGuMwnmJycLF27djWf7b59+8qmTZsceIWoT4QbePU8YX/605/kyJEj8vLLL8v7778vjz/+eJX7/+IXvzDziu3du1f27dsnv//976VJkybmvs8//9z0Dv34xz+Wf/3rX+Z/jvrl+fDDD9fjKwJQHf3MzpkzRz799FOJjIys0ZulwWbt2rWybNky810xbdo0ufvuu2Xnzp282RajTw9e4c0333Tplh41apRs3LixfP3qq6+WZ555Rh544AF54YUXKn2OjIwMmT59uvTo0cOsd+vWzeULUMOP/q+v7D4NToMHD5alS5eKv7+/G18dgJqYNWuWDB8+vMZvVmFhocyePVvee+89iYmJMduuueYa8x+X5cuXm8837ES4gVfQ4SQNGWV0KEq/sDSUaJd1bm6uXLx4UQoKCuTbb7+V5s2bX/YccXFxMnnyZHnllVdk2LBhcuedd8q1115bPmSlPTZ//vOfXcb1tUv75MmTEhERUU+vFEBVBgwYUKs3R0860O+DSwNRUVGR9O/fnzfaYoQbeAUNM2FhYeXrX3zxhdx2223y4IMPSlJSkrRt29b8b2zSpEnmi6uycKNnW/z85z+Xt956y4zd65xT69evl9tvv93U6vzqV78ydTaX6ty5s9tfH4CafQ9cOjR96QxCFy5cKL9dVoOnn/mQkBCX/Zhvzm6EG3glrZnRXpV58+aZLzj12muvfefjwsPDzaLj7hMmTDBnYGm4iYqKkk8++cQlQAHwbHrG5OHDh122HTx4sLyWrmfPnibE6JA0Q1ANCwXF8EoaQvR/aM8//7ycOHHCDDVpwWBVzp8/b4qDU1JSJD09XT744ANTWFw23PS73/1OUlNTzT765fjvf/9b3njjDQqKAQ928803mzMetWBYP7PaG1sx7OgZVr/97W/Nf2b0pAM9cWD//v3me0PXYS/CDbySns45f/58efbZZ6V3796mVkbrb6rSqFEj+e9//yuxsbGm5+anP/2pKUpOSEgw9+uZF3r2xGeffWZOB9fxeL3ORseOHevxVQGojREjRsiTTz5pzpK87rrr5Ny5c+YzXlFiYqLZR78f9D8zelakDlPpqeGwl0/ppQOWAAAAXoyeGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINALe65557xMfH57Ll+PHjbjmezhhf8Tg/+MEP5NZbb5UDBw645XgAPA/hBoDb6Xw+mZmZLsulc/sUFRXV6THfe+89c5x33nlH8vLyzFxiZ8+erdNjAPBMhBsAbufn5yfBwcEuyy233GJmXZ86daq0a9fOTIKodFZnDSItW7aUoKAg+eUvfylnzpwpf66SkhIzCaKGo2bNmplJVDdt2nTZMbXHRo8zYMAAmTt3rmRnZ8s///lPMzP0uHHjzHPrMXTCRQ1CFb3wwgvSrVs38ff3N/v95Cc/Kb9Pj9WnTx9zbD3GsGHDJD8/363vH4DaIdwAcMzLL78sTZs2lQ8++ECWLVtmelZuvvlmMyt7WlqavP322yaU6CzuZTTYrF271ux/5MgRmTZtmtx9991mVveqaBAp6x3SXpzRo0fLjh07zFCV9iqNHTtWMjIyzD563EcffVRmzZolx44dM2246aabzH3aEzRhwgS577775NNPP5WUlBS54447hPmHAc/CrOAA3F5z8+qrr5pekDLaM3P69GnJzc2V/fv3l29/5pln5B//+IcZSirz1VdfSWhoqAkaXbp0kbZt25qelpiYmPJ9Jk+eLN9++62sW7fO1Nxor44Gl379+pnApGFk+/btps5He2Iu1bt3b3nggQdMT9Jf//pXuffee81xW7Vq5bKftjU6OtocQ9sCwDM1droBAOw3dOhQWbp0afl6ixYtTA+IBoWKPv74Y/n73/9uhosupcNJFy5cMCFm+PDhLvdpj4z29lQ0aNAg8fX1NUNG11xzjWzYsMEEG+25efrpp+Wtt94yPTEXL16U8+fPl/fc6HNrcNHHaK+OLrfffrs0b97cDIHpcJoOS+kwmhYq65BVmzZt6vgdA3AlCDcA3E7DTFhYWKXbK9LgoUNEzz777GX7dujQwdTjKA0mISEhl9X1VKRhpmfPnqYupnXr1uXbf/vb35peHK3D0TbpkJUGlLKCZu2t0R4aHXJ69913ZebMmSYM7d271zyPPjY1NdXc9/zzz8uMGTNMLc+lBdIAnEO4AeAxoqKi5PXXX5err75aGje+/OtJw4qGGO1lGTx4cLXPpUNZ11577WXbtb5Hh8q0N6YsUOkwU0V6bC0U1uWpp54yoeb999839TV6evmNN95oFg0+2suzefNmiYuLu+LXD6BuEG4AeIyHHnpIVq5caYasHn/8cVNfo3Uy69evl1WrVpleFe150SJiPWvqhz/8oeTk5JjAEhAQIBMnTvzOY+hZUFpXoz1EGlSefPJJ81xl3nzzTTlx4oQpItbhpq1bt5r7u3fvbnpotBBZh6Pat29v1rV2KCIiws3vDIDaINwA8BgdO3Y0QeV3v/udCRCFhYWmZ0TrXrR+RiUmJspVV11lzprSEKK9Ktrj88QTT9ToGPPnzzcFxlqTo6eg67G0sLmMPp+GHx2KKigoMGHoL3/5i/Tq1cucIbVr1y5ZuHCheYy2bd68eaZAGoDn4GwpAABgFa5zAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACxyf8DHglP3uy05YQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby(df[\"AvgTime\"] > 2)[\"FreePass\"].mean()\n",
    "import seaborn as sns\n",
    "sns.boxplot(x=\"FreePass\", y=\"AvgTime\", data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5fe9495",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Id\", \"FreePass\"])\n",
    "y = df[\"FreePass\"].astype(int)\n",
    "\n",
    "num_cols = X.select_dtypes(include=\"float64\").columns\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols),\n",
    "    (\"cat\", cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "clf_logreg = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_rf = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=100,\n",
    "    max_features=\"sqrt\",\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    "))\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8117c107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\analysis-and-visualization-of-big-data-course\\juliaenv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.4099999999999998\n",
      "Best F1: 0.8033866415804327\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# =====================\n",
    "# 1. Feature engineering\n",
    "# =====================\n",
    "X_fe = X.copy()\n",
    "\n",
    "# Interacciones simples (mejoran LR)\n",
    "X_fe[\"Food_Drinks\"] = X_fe[\"Food\"] + X_fe[\"Drinks\"]\n",
    "X_fe[\"Spending\"] = X_fe[\"Food\"] * X_fe[\"Drinks\"]\n",
    "\n",
    "# Quitamos la variable con leakage\n",
    "X_fe = X_fe.drop(columns=[\"AvgTime\"])\n",
    "\n",
    "# =====================\n",
    "# 2. Columnas\n",
    "# =====================\n",
    "X_fe['Age'] = X_fe['Age'].fillna(X_fe['Age'].median())\n",
    "X_fe['Age_Group'] = pd.cut(X_fe['Age'], \n",
    "                          bins=[0, 25, 35, 45, 55, 100],\n",
    "                          labels=['<25', '25-35', '35-45', '45-55', '55+'])\n",
    "X_fe['Is_Young'] = (X_fe['Age'] < 30).astype(int)\n",
    "X_fe['Is_Senior'] = (X_fe['Age'] > 50).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# 5. Opinion simple\n",
    "X_fe['Opinion_Length'] = X_fe['Opinion'].str.len().fillna(0)\n",
    "X_fe['Has_Opinion'] = X_fe['Opinion'].notna().astype(int)\n",
    "\n",
    "\n",
    "# Sentimiento básico\n",
    "def simple_sentiment(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    text_lower = str(text).lower()\n",
    "    \n",
    "    pos_words = ['good', 'great', 'excellent', 'amazing', 'love', 'best', 'awesome', 'perfect', 'fantastic']\n",
    "    neg_words = ['bad', 'worst', 'terrible', 'awful', 'hate', 'disappointing', 'poor', 'boring', 'waste']\n",
    "    \n",
    "    pos_count = sum(1 for word in pos_words if word in text_lower)\n",
    "    neg_count = sum(1 for word in neg_words if word in text_lower)\n",
    "    \n",
    "    if pos_count > neg_count:\n",
    "        return 1\n",
    "    elif neg_count > pos_count:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "X_fe['Sentiment'] = X['Opinion'].apply(simple_sentiment)\n",
    "X_fe['Sentiment_Score'] = X_fe['Sentiment'].astype(int)\n",
    "\n",
    "\n",
    "# ========= TicketInfo parsing =========\n",
    "X_fe[\"Ticket_Zone\"] = X[\"TicketInfo\"].str.split(\"/\").str[0]\n",
    "X_fe[\"Ticket_Row\"] = X[\"TicketInfo\"].str.split(\"/\").str[1].astype(float)\n",
    "X_fe[\"Ticket_Size\"] = X[\"TicketInfo\"].str.split(\"/\").str[2]\n",
    "X_fe['Is_Front_Row'] = (X_fe['Ticket_Row'] <= 10).astype(int)\n",
    "\n",
    "for col in ['PreferedAlbum', 'Concert', 'Ticket_Zone', 'Ticket_Size']:\n",
    "    X_fe[col] = X_fe[col].fillna('Unknown')\n",
    "\n",
    "for col in X_fe.columns:\n",
    "    if X_fe[col].dtype == 'bool':\n",
    "        X_fe[col] = X_fe[col].astype(int)\n",
    "\n",
    "\n",
    "num_cols = X_fe.select_dtypes(\"float64\").columns\n",
    "cat_cols = X_fe.select_dtypes(\"object\").columns\n",
    "\n",
    "# =====================\n",
    "# 3. Pipelines (LOS TUYOS, sin cambiar filosofía)\n",
    "# =====================\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(\n",
    "        handle_unknown=\"ignore\",\n",
    "        min_frequency=20   # reduce ruido de categorías raras\n",
    "    ))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols),\n",
    "    (\"cat\", cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "selector = SelectFromModel(\n",
    "    LogisticRegression(penalty=\"l1\", solver=\"saga\", C=0.1)\n",
    ")\n",
    "\n",
    "\n",
    "# =====================\n",
    "# 4. Modelo\n",
    "# =====================\n",
    "clf = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"selector\", selector),\n",
    "    (\"model\", LogisticRegression(\n",
    "        max_iter=3000,\n",
    "        solver=\"saga\",\n",
    "        penalty=\"l1\",\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "clf_gb = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", HistGradientBoostingClassifier(\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        max_iter=300,\n",
    "        class_weight=\"balanced\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# =====================\n",
    "# 5. Train / validation\n",
    "# =====================\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_fe,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# =====================\n",
    "# 6. Threshold óptimo\n",
    "# =====================\n",
    "y_probs = clf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "scores = [f1_score(y_val, y_probs > t) for t in thresholds]\n",
    "\n",
    "best_threshold = thresholds[np.argmax(scores)]\n",
    "best_score = max(scores)\n",
    "\n",
    "print(\"Best threshold:\", best_threshold)\n",
    "print(\"Best F1:\", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6347f9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODELO 1: Random Forest\n",
      "============================================================\n",
      "Accuracy: 0.7964\n",
      "F1 Score: 0.7870\n",
      "ROC AUC: 0.8941\n",
      "\n",
      "============================================================\n",
      "MODELO 2: Gradient Boosting\n",
      "============================================================\n",
      "Accuracy: 0.8192\n",
      "F1 Score: 0.8201\n",
      "ROC AUC: 0.9088\n",
      "\n",
      "============================================================\n",
      "MODELO 3: HistGradientBoosting (Recomendado)\n",
      "============================================================\n",
      "Accuracy: 0.8142\n",
      "F1 Score: 0.8144\n",
      "ROC AUC: 0.9077\n",
      "\n",
      "============================================================\n",
      "MODELO 4: k-Nearest Neighbors\n",
      "============================================================\n",
      "Accuracy: 0.8009\n",
      "F1 Score: 0.7945\n",
      "ROC AUC: 0.8739\n",
      "\n",
      "============================================================\n",
      "MODELO 5: Logistic Regression (L1)\n",
      "============================================================\n",
      "Accuracy: 0.7930\n",
      "F1 Score: 0.7878\n",
      "ROC AUC: 0.8800\n",
      "\n",
      "============================================================\n",
      "COMPARATIVA DE MODELOS\n",
      "============================================================\n",
      "               Model  Accuracy  F1 Score  ROC AUC\n",
      "   Gradient Boosting  0.819170  0.820059 0.908773\n",
      "HistGradientBoosting  0.814229  0.814413 0.907704\n",
      "                 KNN  0.800889  0.794493 0.873871\n",
      " Logistic Regression  0.792984  0.787848 0.879960\n",
      "       Random Forest  0.796443  0.786970 0.894134\n",
      "\n",
      "============================================================\n",
      "OPTIMIZACIÓN DE THRESHOLD\n",
      "============================================================\n",
      "Mejor modelo: Gradient Boosting\n",
      "Best threshold: 0.400\n",
      "Best F1 Score: 0.8331\n",
      "\n",
      "Con threshold 0.400:\n",
      "Accuracy: 0.8226\n",
      "F1 Score: 0.8331\n",
      "\n",
      "============================================================\n",
      "VOTING CLASSIFIER (Ensemble)\n",
      "============================================================\n",
      "Voting Classifier Results:\n",
      "Accuracy: 0.8226\n",
      "F1 Score: 0.8238\n",
      "ROC AUC: 0.9100\n",
      "\n",
      "Comparación con Gradient Boosting:\n",
      "  Mejora en F1: 0.0037\n",
      "\n",
      "============================================================\n",
      "ENTRENANDO MODELO FINAL\n",
      "============================================================\n",
      "Usando Voting Classifier como modelo final\n",
      "¡Modelo final entrenado con éxito!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# =====================\n",
    "# 1. Feature engineering (TU VERSIÓN)\n",
    "# =====================\n",
    "X_fe = X.copy()\n",
    "\n",
    "# Interacciones simples (mejoran LR)\n",
    "X_fe[\"Food_Drinks\"] = X_fe[\"Food\"] + X_fe[\"Drinks\"]\n",
    "X_fe[\"Spending\"] = X_fe[\"Food\"] * X_fe[\"Drinks\"]\n",
    "\n",
    "# Quitamos la variable con leakage\n",
    "X_fe = X_fe.drop(columns=[\"AvgTime\"])\n",
    "\n",
    "# =====================\n",
    "# 2. Columnas\n",
    "# =====================\n",
    "X_fe['Age'] = X_fe['Age'].fillna(X_fe['Age'].median())\n",
    "X_fe['Age_Group'] = pd.cut(X_fe['Age'], \n",
    "                          bins=[0, 25, 35, 45, 55, 100],\n",
    "                          labels=['<25', '25-35', '35-45', '45-55', '55+'])\n",
    "X_fe['Is_Young'] = (X_fe['Age'] < 30).astype(int)\n",
    "X_fe['Is_Senior'] = (X_fe['Age'] > 50).astype(int)\n",
    "\n",
    "# Opinion features\n",
    "X_fe['Opinion_Length'] = X_fe['Opinion'].str.len().fillna(0)\n",
    "X_fe['Has_Opinion'] = X_fe['Opinion'].notna().astype(int)\n",
    "\n",
    "# Sentimiento básico\n",
    "def simple_sentiment(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    text_lower = str(text).lower()\n",
    "    \n",
    "    pos_words = ['good', 'great', 'excellent', 'amazing', 'love', 'best', 'awesome', 'perfect', 'fantastic']\n",
    "    neg_words = ['bad', 'worst', 'terrible', 'awful', 'hate', 'disappointing', 'poor', 'boring', 'waste']\n",
    "    \n",
    "    pos_count = sum(1 for word in pos_words if word in text_lower)\n",
    "    neg_count = sum(1 for word in neg_words if word in text_lower)\n",
    "    \n",
    "    if pos_count > neg_count:\n",
    "        return 1\n",
    "    elif neg_count > pos_count:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "X_fe['Sentiment'] = X['Opinion'].apply(simple_sentiment)\n",
    "X_fe['Sentiment_Score'] = X_fe['Sentiment'].astype(int)\n",
    "\n",
    "# TicketInfo parsing\n",
    "X_fe[\"Ticket_Zone\"] = X[\"TicketInfo\"].str.split(\"/\").str[0]\n",
    "X_fe[\"Ticket_Row\"] = X[\"TicketInfo\"].str.split(\"/\").str[1].astype(float)\n",
    "X_fe[\"Ticket_Size\"] = X[\"TicketInfo\"].str.split(\"/\").str[2]\n",
    "X_fe['Is_Front_Row'] = (X_fe['Ticket_Row'] <= 10).astype(int)\n",
    "\n",
    "# Añadir más features de ticket\n",
    "X_fe['Is_VIP_Zone'] = X_fe['Ticket_Zone'].str.contains('VIP', case=False, na=False).astype(int)\n",
    "\n",
    "# Imputar valores faltantes\n",
    "# for col in ['PreferedAlbum', 'Concert', 'Ticket_Zone', 'Ticket_Size', 'Age_Group']:\n",
    "#     X_fe[col] = X_fe[col].fillna('Unknown')\n",
    "\n",
    "# for col in ['Vinyl', 'VIP']:\n",
    "#     X_fe[col] = X_fe[col].fillna('Unknown')\n",
    "\n",
    "# Asegurar tipos\n",
    "for col in X_fe.columns:\n",
    "    if X_fe[col].dtype == 'bool':\n",
    "        X_fe[col] = X_fe[col].astype(int)\n",
    "\n",
    "# =====================\n",
    "# 3. Pipelines CORREGIDOS (matriz densa)\n",
    "# =====================\n",
    "num_cols = X_fe.select_dtypes(\"float64\").columns\n",
    "cat_cols = X_fe.select_dtypes(\"object\").columns\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# OneHotEncoder CON sparse_output=False para matriz densa\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(\n",
    "        handle_unknown=\"ignore\",\n",
    "        min_frequency=20,\n",
    "        sparse_output=False  # <--- ESTA ES LA CLAVE\n",
    "    ))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols),\n",
    "    (\"cat\", cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "# =====================\n",
    "# 4. PROBAR MODELOS (todos compatibles con matrices densas)\n",
    "# =====================\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Dividir datos\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_fe, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# =====================\n",
    "# MODELO 1: Random Forest (funciona bien)\n",
    "# =====================\n",
    "print(\"=\" * 60)\n",
    "print(\"MODELO 1: Random Forest\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "clf_rf = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=15,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "clf_rf.fit(X_train, y_train)\n",
    "y_pred_rf = clf_rf.predict(X_val)\n",
    "y_proba_rf = clf_rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_pred_rf):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_val, y_pred_rf):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_val, y_proba_rf):.4f}\")\n",
    "\n",
    "# =====================\n",
    "# MODELO 2: Gradient Boosting (Sklearn)\n",
    "# =====================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODELO 2: Gradient Boosting\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "clf_gb = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", GradientBoostingClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        subsample=0.8\n",
    "    ))\n",
    "])\n",
    "\n",
    "clf_gb.fit(X_train, y_train)\n",
    "y_pred_gb = clf_gb.predict(X_val)\n",
    "y_proba_gb = clf_gb.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_pred_gb):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_val, y_pred_gb):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_val, y_proba_gb):.4f}\")\n",
    "\n",
    "# =====================\n",
    "# MODELO 3: HistGradientBoosting (OPTIMIZADO)\n",
    "# =====================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODELO 3: HistGradientBoosting (Recomendado)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "clf_hgb = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", HistGradientBoostingClassifier(\n",
    "        max_iter=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        min_samples_leaf=20,\n",
    "        l2_regularization=0.1,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        random_state=42,\n",
    "        class_weight='balanced'\n",
    "    ))\n",
    "])\n",
    "\n",
    "clf_hgb.fit(X_train, y_train)\n",
    "y_pred_hgb = clf_hgb.predict(X_val)\n",
    "y_proba_hgb = clf_hgb.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_pred_hgb):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_val, y_pred_hgb):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_val, y_proba_hgb):.4f}\")\n",
    "\n",
    "# =====================\n",
    "# MODELO 4: KNeighbors (funciona con densas)\n",
    "# =====================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODELO 4: k-Nearest Neighbors\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "clf_knn = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", KNeighborsClassifier(\n",
    "        n_neighbors=50,\n",
    "        weights='distance',\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "clf_knn.fit(X_train, y_train)\n",
    "y_pred_knn = clf_knn.predict(X_val)\n",
    "y_proba_knn = clf_knn.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_pred_knn):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_val, y_pred_knn):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_val, y_proba_knn):.4f}\")\n",
    "\n",
    "# =====================\n",
    "# MODELO 5: Logistic Regression (tu original)\n",
    "# =====================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODELO 5: Logistic Regression (L1)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "clf_lr = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"selector\", SelectFromModel(\n",
    "        LogisticRegression(penalty=\"l1\", solver=\"saga\", C=0.1, max_iter=1000)\n",
    "    )),\n",
    "    (\"model\", LogisticRegression(\n",
    "        max_iter=3000,\n",
    "        solver=\"saga\",\n",
    "        penalty=\"l1\",\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "clf_lr.fit(X_train, y_train)\n",
    "y_pred_lr = clf_lr.predict(X_val)\n",
    "y_proba_lr = clf_lr.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_pred_lr):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_val, y_pred_lr):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_val, y_proba_lr):.4f}\")\n",
    "\n",
    "# =====================\n",
    "# COMPARATIVA\n",
    "# =====================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARATIVA DE MODELOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "for name, y_pred, y_proba in [\n",
    "    ('Random Forest', y_pred_rf, y_proba_rf),\n",
    "    ('Gradient Boosting', y_pred_gb, y_proba_gb),\n",
    "    ('HistGradientBoosting', y_pred_hgb, y_proba_hgb),\n",
    "    ('KNN', y_pred_knn, y_proba_knn),\n",
    "    ('Logistic Regression', y_pred_lr, y_proba_lr)\n",
    "]:\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_val, y_pred),\n",
    "        'F1 Score': f1_score(y_val, y_pred),\n",
    "        'ROC AUC': roc_auc_score(y_val, y_proba)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('F1 Score', ascending=False)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# =====================\n",
    "# THRESHOLD OPTIMIZATION PARA EL MEJOR MODELO\n",
    "# =====================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OPTIMIZACIÓN DE THRESHOLD\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Seleccionar el mejor modelo\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "print(f\"Mejor modelo: {best_model_name}\")\n",
    "\n",
    "if best_model_name == 'HistGradientBoosting':\n",
    "    best_clf = clf_hgb\n",
    "    y_probs = y_proba_hgb\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_clf = clf_rf\n",
    "    y_probs = y_proba_rf\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    best_clf = clf_gb\n",
    "    y_probs = y_proba_gb\n",
    "elif best_model_name == 'Logistic Regression':\n",
    "    best_clf = clf_lr\n",
    "    y_probs = y_proba_lr\n",
    "else:\n",
    "    best_clf = clf_knn\n",
    "    y_probs = y_proba_knn\n",
    "\n",
    "# Buscar mejor threshold\n",
    "thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "scores = [f1_score(y_val, y_probs > t) for t in thresholds]\n",
    "\n",
    "best_threshold = thresholds[np.argmax(scores)]\n",
    "best_score = max(scores)\n",
    "\n",
    "print(f\"Best threshold: {best_threshold:.3f}\")\n",
    "print(f\"Best F1 Score: {best_score:.4f}\")\n",
    "\n",
    "# Resultados con threshold óptimo\n",
    "y_pred_optimal = (y_probs > best_threshold).astype(int)\n",
    "print(f\"\\nCon threshold {best_threshold:.3f}:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_pred_optimal):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_val, y_pred_optimal):.4f}\")\n",
    "\n",
    "# =====================\n",
    "# VOTING CLASSIFIER (opcional)\n",
    "# =====================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VOTING CLASSIFIER (Ensemble)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Tomar los 3 mejores modelos\n",
    "top_3_models = results_df.head(3)['Model'].values\n",
    "\n",
    "estimators = []\n",
    "for model_name in top_3_models:\n",
    "    if model_name == 'HistGradientBoosting':\n",
    "        estimators.append(('hgb', HistGradientBoostingClassifier(\n",
    "            max_iter=300, learning_rate=0.05, max_depth=7,\n",
    "            min_samples_leaf=20, random_state=42, class_weight='balanced'\n",
    "        )))\n",
    "    elif model_name == 'Random Forest':\n",
    "        estimators.append(('rf', RandomForestClassifier(\n",
    "            n_estimators=200, max_depth=15, class_weight='balanced',\n",
    "            random_state=42, n_jobs=-1\n",
    "        )))\n",
    "    elif model_name == 'Gradient Boosting':\n",
    "        estimators.append(('gb', GradientBoostingClassifier(\n",
    "            n_estimators=200, learning_rate=0.05, max_depth=5,\n",
    "            random_state=42\n",
    "        )))\n",
    "\n",
    "if estimators:\n",
    "    voting_clf = Pipeline([\n",
    "        (\"prep\", preprocessor),\n",
    "        (\"voting\", VotingClassifier(\n",
    "            estimators=estimators,\n",
    "            voting='soft',\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    voting_clf.fit(X_train, y_train)\n",
    "    y_pred_voting = voting_clf.predict(X_val)\n",
    "    y_proba_voting = voting_clf.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    print(f\"Voting Classifier Results:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_val, y_pred_voting):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_val, y_pred_voting):.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc_score(y_val, y_proba_voting):.4f}\")\n",
    "    \n",
    "    # Comparar con el mejor modelo individual\n",
    "    print(f\"\\nComparación con {best_model_name}:\")\n",
    "    print(f\"  Mejora en F1: {f1_score(y_val, y_pred_voting) - results_df.iloc[0]['F1 Score']:.4f}\")\n",
    "\n",
    "# =====================\n",
    "# ENTRENAR MODELO FINAL\n",
    "# =====================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ENTRENANDO MODELO FINAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Usar el mejor modelo individual o el voting\n",
    "if 'voting_clf' in locals() and f1_score(y_val, y_pred_voting) > results_df.iloc[0]['F1 Score']:\n",
    "    final_model = voting_clf\n",
    "    print(\"Usando Voting Classifier como modelo final\")\n",
    "else:\n",
    "    final_model = best_clf\n",
    "    print(f\"Usando {best_model_name} como modelo final\")\n",
    "\n",
    "# Entrenar con todos los datos\n",
    "final_model.fit(X_fe, y)\n",
    "print(\"¡Modelo final entrenado con éxito!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2218436d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando datos de entrenamiento...\n",
      "Train - Columnas numéricas: 7\n",
      "Train - Columnas categóricas: 9\n",
      "Train - Total columnas: 16\n",
      "\n",
      "Entrenando modelo...\n",
      "F1 Score en validation: 0.8253\n",
      "Accuracy en validation: 0.8241\n",
      "\n",
      "Preparando datos de test...\n",
      "Test - Total columnas: 20\n",
      "Test después de ajuste - Total columnas: 20\n",
      "\n",
      "Haciendo predicciones...\n",
      "\n",
      "Submission creado:\n",
      "Número de predicciones: 2854\n",
      "FreePass positivos: 1379 (48.3%)\n",
      "Guardado como 'submission_final.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# =====================\n",
    "# 1. FUNCIÓN PARA PREPARAR DATOS (USADA EN TRAIN Y TEST)\n",
    "# =====================\n",
    "def prepare_data(df, is_train=True):\n",
    "    \"\"\"Prepara datos de manera consistente para train y test\"\"\"\n",
    "    df_fe = df.copy()\n",
    "    \n",
    "    # Eliminar columnas que no queremos\n",
    "    if 'AvgTime' in df_fe.columns:\n",
    "        df_fe = df_fe.drop(columns=['AvgTime'])\n",
    "    \n",
    "    # Guardar Id si existe (para submission)\n",
    "    if 'Id' in df_fe.columns and is_train == False:\n",
    "        ids = df_fe['Id']\n",
    "        df_fe = df_fe.drop(columns=['Id'])\n",
    "    else:\n",
    "        ids = None\n",
    "    \n",
    "    # Features básicas\n",
    "    df_fe[\"Food_Drinks\"] = df_fe[\"Food\"] + df_fe[\"Drinks\"]\n",
    "    df_fe[\"Spending\"] = df_fe[\"Food\"] * df_fe[\"Drinks\"]\n",
    "    \n",
    "    # Age features\n",
    "    df_fe['Age'] = df_fe['Age'].fillna(df_fe['Age'].median())\n",
    "    df_fe['Is_Young'] = (df_fe['Age'] < 5).astype(int)\n",
    "    # df_fe['Is_Senior'] = (df_fe['Age'] > 50).astype(int)\n",
    "    \n",
    "    # Opinion features\n",
    "    df_fe['Opinion_Length'] = df_fe['Opinion'].str.len().fillna(0)\n",
    "    df_fe['Has_Opinion'] = df_fe['Opinion'].notna().astype(int)\n",
    "    \n",
    "    # TicketInfo\n",
    "    df_fe[\"Ticket_Zone\"] = df_fe[\"TicketInfo\"].str.split(\"/\").str[0]\n",
    "    df_fe[\"Ticket_Row\"] = pd.to_numeric(df_fe[\"TicketInfo\"].str.split(\"/\").str[1], errors='coerce')\n",
    "    df_fe[\"Ticket_Size\"] = df_fe[\"TicketInfo\"].str.split(\"/\").str[2]\n",
    "    df_fe['Is_Front_Row'] = (df_fe['Ticket_Row'] <= 10).astype(int)\n",
    "    df_fe['Is_VIP_Zone'] = df_fe['Ticket_Zone'].str.contains('VIP', case=False, na=False).astype(int)\n",
    "    \n",
    "    # Age Group\n",
    "    df_fe['Age_Group'] = pd.cut(df_fe['Age'], \n",
    "                               bins=[0, 25, 35, 45, 55, 100],\n",
    "                               labels=['<25', '25-35', '35-45', '45-55', '55+'])\n",
    "    \n",
    "    # Imputar categóricas\n",
    "    # for col in ['PreferedAlbum', 'Concert', 'Ticket_Zone', 'Ticket_Size', 'Age_Group']:\n",
    "    #     df_fe[col] = df_fe[col].fillna('Unknown')\n",
    "    \n",
    "    # for col in ['Vinyl', 'VIP']:\n",
    "    #     df_fe[col] = df_fe[col].fillna('Unknown')\n",
    "\n",
    "    # if 'Opinion' in df_fe.columns:\n",
    "    #     df_fe = df_fe.drop(columns=['Opinion'])\n",
    "\n",
    "    # if 'TicketInfo' in df_fe.columns:\n",
    "    #     df_fe = df_fe.drop(columns=['TicketInfo'])\n",
    "\n",
    "    # Asegurar tipos\n",
    "    for col in df_fe.columns:\n",
    "        if df_fe[col].dtype == 'bool':\n",
    "            df_fe[col] = df_fe[col].astype(int)\n",
    "    \n",
    "    return df_fe, ids\n",
    "\n",
    "# =====================\n",
    "# 2. PREPARAR DATOS DE TRAIN\n",
    "# =====================\n",
    "print(\"Preparando datos de entrenamiento...\")\n",
    "X_fe, _ = prepare_data(X, is_train=True)\n",
    "\n",
    "# Identificar columnas\n",
    "num_cols = X_fe.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "cat_cols = X_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Train - Columnas numéricas: {len(num_cols)}\")\n",
    "print(f\"Train - Columnas categóricas: {len(cat_cols)}\")\n",
    "print(f\"Train - Total columnas: {len(num_cols) + len(cat_cols)}\")\n",
    "\n",
    "# =====================\n",
    "# 3. CREAR PIPELINE\n",
    "# =====================\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(\n",
    "        handle_unknown=\"ignore\",\n",
    "        sparse_output=False\n",
    "    ))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols),\n",
    "    (\"cat\", cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "# Pipeline completo\n",
    "pipeline = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", HistGradientBoostingClassifier(\n",
    "        max_iter=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        min_samples_leaf=20,\n",
    "        l2_regularization=0.1,\n",
    "        early_stopping=True,\n",
    "        random_state=42,\n",
    "        class_weight='balanced'\n",
    "    ))\n",
    "])\n",
    "\n",
    "# =====================\n",
    "# 4. ENTRENAR MODELO\n",
    "# =====================\n",
    "print(\"\\nEntrenando modelo...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_fe, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "y_pred = pipeline.predict(X_val)\n",
    "print(f\"F1 Score en validation: {f1_score(y_val, y_pred):.4f}\")\n",
    "print(f\"Accuracy en validation: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "\n",
    "# =====================\n",
    "# 5. PREPARAR Y PREDECIR TEST\n",
    "# =====================\n",
    "print(\"\\nPreparando datos de test...\")\n",
    "X_test_fe, test_ids = prepare_data(df_test, is_train=False)\n",
    "\n",
    "# Verificar que test tenga las mismas columnas que train\n",
    "print(f\"Test - Total columnas: {X_test_fe.shape[1]}\")\n",
    "\n",
    "# Asegurar que test tiene las mismas columnas en el mismo orden\n",
    "missing_cols = set(X_fe.columns) - set(X_test_fe.columns)\n",
    "extra_cols = set(X_test_fe.columns) - set(X_fe.columns)\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"Columnas faltantes en test: {missing_cols}\")\n",
    "    # Añadir columnas faltantes con valores por defecto\n",
    "    for col in missing_cols:\n",
    "        X_test_fe[col] = 0 if X_fe[col].dtype in ['int64', 'float64'] else 'Unknown'\n",
    "\n",
    "if extra_cols:\n",
    "    print(f\"Columnas extra en test: {extra_cols}\")\n",
    "    # Eliminar columnas extra\n",
    "    X_test_fe = X_test_fe.drop(columns=list(extra_cols))\n",
    "\n",
    "# Reordenar columnas para que coincidan con train\n",
    "X_test_fe = X_test_fe[X_fe.columns]\n",
    "\n",
    "print(f\"Test después de ajuste - Total columnas: {X_test_fe.shape[1]}\")\n",
    "\n",
    "# =====================\n",
    "# 6. HACER PREDICCIONES\n",
    "# =====================\n",
    "print(\"\\nHaciendo predicciones...\")\n",
    "y_proba_test = pipeline.predict_proba(X_test_fe)[:, 1]\n",
    "\n",
    "# Aplicar threshold (ajusta según lo que optimizaste)\n",
    "best_threshold = 0.5\n",
    "preds = (y_proba_test > best_threshold).astype(int)\n",
    "\n",
    "# =====================\n",
    "# 7. CREAR SUBMISSION\n",
    "# =====================\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": test_ids,\n",
    "    \"FreePass\": preds.astype(bool)\n",
    "})\n",
    "\n",
    "print(f\"\\nSubmission creado:\")\n",
    "print(f\"Número de predicciones: {len(submission)}\")\n",
    "print(f\"FreePass positivos: {submission['FreePass'].sum()} ({submission['FreePass'].mean()*100:.1f}%)\")\n",
    "\n",
    "# Guardar\n",
    "submission.to_csv(\"submission_final.csv\", index=False)\n",
    "print(\"Guardado como 'submission_final.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd90030e",
   "metadata": {},
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6f9965d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando datos de entrenamiento...\n",
      "Train - Columnas numéricas: 7\n",
      "Train - Columnas categóricas: 9\n",
      "Train - Total columnas: 16\n",
      "\n",
      "Entrenando modelo base (rápido)...\n",
      "F1 Score en validation: 0.8253\n",
      "Accuracy en validation: 0.8241\n",
      "Best threshold: 0.322 | Best F1: 0.8318\n",
      "\n",
      "Preparando datos de test...\n",
      "Test - Total columnas: 20\n",
      "Test después de ajuste - Total columnas: 20\n",
      "\n",
      "Haciendo predicciones...\n",
      "\n",
      "Submission creado:\n",
      "Número de predicciones: 2854\n",
      "FreePass positivos: 1765 (61.8%)\n",
      "Guardado como 'submission_final.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# =====================\n",
    "# 1. FUNCIÓN PARA PREPARAR DATOS (USADA EN TRAIN Y TEST)\n",
    "# =====================\n",
    "def prepare_data(df, is_train=True):\n",
    "    \"\"\"Prepara datos de manera consistente para train y test\"\"\"\n",
    "    df_fe = df.copy()\n",
    "    \n",
    "    # Eliminar columnas que no queremos\n",
    "    if 'AvgTime' in df_fe.columns:\n",
    "        df_fe = df_fe.drop(columns=['AvgTime'])\n",
    "    \n",
    "    # Guardar Id si existe (para submission)\n",
    "    if 'Id' in df_fe.columns and is_train == False:\n",
    "        ids = df_fe['Id']\n",
    "        df_fe = df_fe.drop(columns=['Id'])\n",
    "    else:\n",
    "        ids = None\n",
    "    \n",
    "    # Features básicas\n",
    "    df_fe[\"Food_Drinks\"] = df_fe[\"Food\"] + df_fe[\"Drinks\"]\n",
    "    df_fe[\"Spending\"] = df_fe[\"Food\"] * df_fe[\"Drinks\"]\n",
    "    \n",
    "    # Age features\n",
    "    df_fe['Age'] = df_fe['Age'].fillna(df_fe['Age'].median())\n",
    "    df_fe['Is_Young'] = (df_fe['Age'] < 5).astype(int)\n",
    "    # df_fe['Is_Senior'] = (df_fe['Age'] > 50).astype(int)\n",
    "    \n",
    "    # Opinion features\n",
    "    df_fe['Opinion_Length'] = df_fe['Opinion'].str.len().fillna(0)\n",
    "    df_fe['Has_Opinion'] = df_fe['Opinion'].notna().astype(int)\n",
    "    \n",
    "    # TicketInfo\n",
    "    df_fe[\"Ticket_Zone\"] = df_fe[\"TicketInfo\"].str.split(\"/\").str[0]\n",
    "    df_fe[\"Ticket_Row\"] = pd.to_numeric(df_fe[\"TicketInfo\"].str.split(\"/\").str[1], errors='coerce')\n",
    "    df_fe[\"Ticket_Size\"] = df_fe[\"TicketInfo\"].str.split(\"/\").str[2]\n",
    "    df_fe['Is_Front_Row'] = (df_fe['Ticket_Row'] <= 10).astype(int)\n",
    "    df_fe['Is_VIP_Zone'] = df_fe['Ticket_Zone'].str.contains('VIP', case=False, na=False).astype(int)\n",
    "    \n",
    "    # Age Group\n",
    "    df_fe['Age_Group'] = pd.cut(df_fe['Age'], \n",
    "                               bins=[0, 25, 35, 45, 55, 100],\n",
    "                               labels=['<25', '25-35', '35-45', '45-55', '55+'])\n",
    "    \n",
    "    # Imputar categóricas\n",
    "    # for col in ['PreferedAlbum', 'Concert', 'Ticket_Zone', 'Ticket_Size', 'Age_Group']:\n",
    "    #     df_fe[col] = df_fe[col].fillna('Unknown')\n",
    "    \n",
    "    # for col in ['Vinyl', 'VIP']:\n",
    "    #     df_fe[col] = df_fe[col].fillna('Unknown')\n",
    "\n",
    "    # if 'Opinion' in df_fe.columns:\n",
    "    #     df_fe = df_fe.drop(columns=['Opinion'])\n",
    "\n",
    "    # if 'TicketInfo' in df_fe.columns:\n",
    "    #     df_fe = df_fe.drop(columns=['TicketInfo'])\n",
    "\n",
    "    # Asegurar tipos\n",
    "    for col in df_fe.columns:\n",
    "        if df_fe[col].dtype == 'bool':\n",
    "            df_fe[col] = df_fe[col].astype(int)\n",
    "    \n",
    "    return df_fe, ids\n",
    "\n",
    "# =====================\n",
    "# 2. PREPARAR DATOS DE TRAIN\n",
    "# =====================\n",
    "print(\"Preparando datos de entrenamiento...\")\n",
    "X_fe, _ = prepare_data(X, is_train=True)\n",
    "\n",
    "# Identificar columnas\n",
    "num_cols = X_fe.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "cat_cols = X_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Train - Columnas numéricas: {len(num_cols)}\")\n",
    "print(f\"Train - Columnas categóricas: {len(cat_cols)}\")\n",
    "print(f\"Train - Total columnas: {len(num_cols) + len(cat_cols)}\")\n",
    "\n",
    "# =====================\n",
    "# 3. CREAR PIPELINE\n",
    "# =====================\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(\n",
    "        handle_unknown=\"ignore\",\n",
    "        sparse_output=False\n",
    "    ))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols),\n",
    "    (\"cat\", cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "# Pipeline completo\n",
    "pipeline = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", HistGradientBoostingClassifier(\n",
    "        max_iter=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        min_samples_leaf=20,\n",
    "        l2_regularization=0.1,\n",
    "        early_stopping=True,\n",
    "        random_state=42,\n",
    "        class_weight='balanced'\n",
    "    ))\n",
    "])\n",
    "\n",
    "# =====================\n",
    "# 4. ENTRENAR MODELO\n",
    "# =====================\n",
    "print(\"\\nEntrenando modelo base (rápido)...\")\n",
    "\n",
    "# Split train / validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_fe, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Entrenar pipeline original\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar en validation\n",
    "y_pred = pipeline.predict(X_val)\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "print(f\"F1 Score en validation: {f1_score(y_val, y_pred):.4f}\")\n",
    "print(f\"Accuracy en validation: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "\n",
    "# Optimizar threshold\n",
    "import numpy as np\n",
    "probs = pipeline.predict_proba(X_val)[:, 1]\n",
    "thresholds = np.linspace(0.2, 0.8, 50)\n",
    "\n",
    "best_t, best_f1 = 0.5, 0\n",
    "for t in thresholds:\n",
    "    preds = (probs > t).astype(int)\n",
    "    f1 = f1_score(y_val, preds)\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_t = f1, t\n",
    "\n",
    "print(f\"Best threshold: {best_t:.3f} | Best F1: {best_f1:.4f}\")\n",
    "\n",
    "# =====================\n",
    "# 5. PREPARAR Y PREDECIR TEST\n",
    "# =====================\n",
    "print(\"\\nPreparando datos de test...\")\n",
    "X_test_fe, test_ids = prepare_data(df_test, is_train=False)\n",
    "\n",
    "# Verificar que test tenga las mismas columnas que train\n",
    "print(f\"Test - Total columnas: {X_test_fe.shape[1]}\")\n",
    "\n",
    "# Asegurar que test tiene las mismas columnas en el mismo orden\n",
    "missing_cols = set(X_fe.columns) - set(X_test_fe.columns)\n",
    "extra_cols = set(X_test_fe.columns) - set(X_fe.columns)\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"Columnas faltantes en test: {missing_cols}\")\n",
    "    # Añadir columnas faltantes con valores por defecto\n",
    "    for col in missing_cols:\n",
    "        X_test_fe[col] = 0 if X_fe[col].dtype in ['int64', 'float64'] else 'Unknown'\n",
    "\n",
    "if extra_cols:\n",
    "    print(f\"Columnas extra en test: {extra_cols}\")\n",
    "    # Eliminar columnas extra\n",
    "    X_test_fe = X_test_fe.drop(columns=list(extra_cols))\n",
    "\n",
    "# Reordenar columnas para que coincidan con train\n",
    "X_test_fe = X_test_fe[X_fe.columns]\n",
    "\n",
    "print(f\"Test después de ajuste - Total columnas: {X_test_fe.shape[1]}\")\n",
    "\n",
    "# =====================\n",
    "# 6. HACER PREDICCIONES\n",
    "# =====================\n",
    "print(\"\\nHaciendo predicciones...\")\n",
    "y_proba_test = pipeline.predict_proba(X_test_fe)[:, 1]\n",
    "\n",
    "# Aplicar threshold (ajusta según lo que optimizaste)\n",
    "best_threshold = best_t\n",
    "preds = (y_proba_test > best_threshold).astype(int)\n",
    "\n",
    "# =====================\n",
    "# 7. CREAR SUBMISSION\n",
    "# =====================\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": test_ids,\n",
    "    \"FreePass\": preds.astype(bool)\n",
    "})\n",
    "\n",
    "print(f\"\\nSubmission creado:\")\n",
    "print(f\"Número de predicciones: {len(submission)}\")\n",
    "print(f\"FreePass positivos: {submission['FreePass'].sum()} ({submission['FreePass'].mean()*100:.1f}%)\")\n",
    "\n",
    "# Guardar\n",
    "submission.to_csv(\"submission_final.csv\", index=False)\n",
    "print(\"Guardado como 'submission_final.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39ce31dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando datos de entrenamiento...\n",
      "Train - Num cols: 7, Cat cols: 9, Total: 16\n",
      "Best threshold: 0.322, Best F1: 0.8318\n",
      "Submission ensemble guardada como submission_ensemble.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# =====================\n",
    "# 1. FUNCIÓN PARA PREPARAR DATOS\n",
    "# =====================\n",
    "def prepare_data(df, is_train=True):\n",
    "    df_fe = df.copy()\n",
    "    if 'AvgTime' in df_fe.columns:\n",
    "        df_fe = df_fe.drop(columns=['AvgTime'])\n",
    "    if 'Id' in df_fe.columns and not is_train:\n",
    "        ids = df_fe['Id']\n",
    "        df_fe = df_fe.drop(columns=['Id'])\n",
    "    else:\n",
    "        ids = None\n",
    "    # Features\n",
    "    df_fe[\"Food_Drinks\"] = df_fe[\"Food\"] + df_fe[\"Drinks\"]\n",
    "    df_fe[\"Spending\"] = df_fe[\"Food\"] * df_fe[\"Drinks\"]\n",
    "    df_fe['Age'] = df_fe['Age'].fillna(df_fe['Age'].median())\n",
    "    df_fe['Is_Young'] = (df_fe['Age'] < 5).astype(int)\n",
    "    df_fe['Opinion_Length'] = df_fe['Opinion'].str.len().fillna(0)\n",
    "    df_fe['Has_Opinion'] = df_fe['Opinion'].notna().astype(int)\n",
    "    df_fe[\"Ticket_Zone\"] = df_fe[\"TicketInfo\"].str.split(\"/\").str[0]\n",
    "    df_fe[\"Ticket_Row\"] = pd.to_numeric(df_fe[\"TicketInfo\"].str.split(\"/\").str[1], errors='coerce')\n",
    "    df_fe[\"Ticket_Size\"] = df_fe[\"TicketInfo\"].str.split(\"/\").str[2]\n",
    "    df_fe['Is_Front_Row'] = (df_fe['Ticket_Row'] <= 10).astype(int)\n",
    "    df_fe['Is_VIP_Zone'] = df_fe['Ticket_Zone'].str.contains('VIP', case=False, na=False).astype(int)\n",
    "    df_fe['Age_Group'] = pd.cut(df_fe['Age'], bins=[0,25,35,45,55,100], labels=['<25','25-35','35-45','45-55','55+'])\n",
    "    for col in df_fe.columns:\n",
    "        if df_fe[col].dtype == 'bool':\n",
    "            df_fe[col] = df_fe[col].astype(int)\n",
    "    return df_fe, ids\n",
    "\n",
    "# =====================\n",
    "# 2. PREPARAR TRAIN\n",
    "# =====================\n",
    "print(\"Preparando datos de entrenamiento...\")\n",
    "X_fe, _ = prepare_data(X, is_train=True)\n",
    "num_cols = X_fe.select_dtypes(include=['float64','int64']).columns.tolist()\n",
    "cat_cols = X_fe.select_dtypes(include=['object','category']).columns.tolist()\n",
    "print(f\"Train - Num cols: {len(num_cols)}, Cat cols: {len(cat_cols)}, Total: {len(num_cols)+len(cat_cols)}\")\n",
    "\n",
    "# =====================\n",
    "# 3. PIPELINE\n",
    "# =====================\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols),\n",
    "    (\"cat\", cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "# =====================\n",
    "# 4. MODELO BASE (ganador)\n",
    "# =====================\n",
    "pipeline_1 = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", HistGradientBoostingClassifier(\n",
    "        max_iter=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        min_samples_leaf=20,\n",
    "        l2_regularization=0.1,\n",
    "        early_stopping=True,\n",
    "        random_state=42,\n",
    "        class_weight='balanced'\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Validación rápida para threshold\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_fe, y, test_size=0.2, stratify=y, random_state=42)\n",
    "pipeline_1.fit(X_train, y_train)\n",
    "probs_val_1 = pipeline_1.predict_proba(X_val)[:,1]\n",
    "\n",
    "thresholds = np.linspace(0.2,0.8,50)\n",
    "best_t, best_f1 = 0.5, 0\n",
    "for t in thresholds:\n",
    "    preds = (probs_val_1 > t).astype(int)\n",
    "    f1 = f1_score(y_val, preds)\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_t = f1, t\n",
    "print(f\"Best threshold: {best_t:.3f}, Best F1: {best_f1:.4f}\")\n",
    "\n",
    "# Entrenar modelo final con TODO el train\n",
    "pipeline_1.fit(X_fe, y)\n",
    "probs_model_1 = pipeline_1.predict_proba(X_fe)[:,1]\n",
    "\n",
    "# =====================\n",
    "# 5. SEGUNDO MODELO (diverso para ensemble)\n",
    "# =====================\n",
    "pipeline_2 = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", HistGradientBoostingClassifier(\n",
    "        max_iter=250,\n",
    "        learning_rate=0.08,\n",
    "        max_depth=5,\n",
    "        min_samples_leaf=40,\n",
    "        l2_regularization=0.2,\n",
    "        early_stopping=True,\n",
    "        random_state=99,\n",
    "        class_weight='balanced'\n",
    "    ))\n",
    "])\n",
    "pipeline_2.fit(X_fe, y)\n",
    "probs_model_2 = pipeline_2.predict_proba(X_fe)[:,1]\n",
    "\n",
    "# =====================\n",
    "# 6. PREPARAR TEST\n",
    "# =====================\n",
    "X_test_fe, test_ids = prepare_data(df_test, is_train=False)\n",
    "missing_cols = set(X_fe.columns) - set(X_test_fe.columns)\n",
    "for col in missing_cols:\n",
    "    X_test_fe[col] = 0 if X_fe[col].dtype in ['int64','float64'] else 'Unknown'\n",
    "X_test_fe = X_test_fe[X_fe.columns]\n",
    "\n",
    "# =====================\n",
    "# 7. ENSEMBLE + SUBMISSION\n",
    "# =====================\n",
    "probs_ensemble = 0.6* pipeline_1.predict_proba(X_test_fe)[:,1] + 0.4* pipeline_2.predict_proba(X_test_fe)[:,1]\n",
    "preds_ensemble = (probs_ensemble > best_t).astype(bool)\n",
    "\n",
    "submission_ensemble = pd.DataFrame({\n",
    "    \"Id\": test_ids,\n",
    "    \"FreePass\": preds_ensemble\n",
    "})\n",
    "submission_ensemble.to_csv(\"submission_ensemble.csv\", index=False)\n",
    "print(\"Submission ensemble guardada como submission_ensemble.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b335cca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando datos...\n",
      "Num cols: 8, Cat cols: 5, Text col: 1\n",
      "Entrenando con Cross-Validation y optimizando Threshold...\n",
      "CV F1 Score estimado: 0.8237 | Mejor Threshold: 0.352\n",
      "Re-entrenando modelo final con TODO el dataset...\n",
      "¡Hecho! Archivo guardado: submission_nlp_ensemble_fixed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# =====================\n",
    "# 1. FUNCIÓN DE LIMPIEZA DE TEXTO\n",
    "# =====================\n",
    "def clean_text(series):\n",
    "    # Rellenar nulos, minúsculas y quitar caracteres no alfabéticos\n",
    "    return series.fillna(\"\").astype(str).str.lower().str.replace(r'[^a-z\\s]', '', regex=True)\n",
    "\n",
    "# =====================\n",
    "# 2. FEATURE ENGINEERING MEJORADO\n",
    "# =====================\n",
    "def prepare_data(df, is_train=True):\n",
    "    df_fe = df.copy()\n",
    "    \n",
    "    # Eliminar columnas irrelevantes si existen\n",
    "    if 'AvgTime' in df_fe.columns: df_fe = df_fe.drop(columns=['AvgTime'])\n",
    "    \n",
    "    ids = None\n",
    "    if 'Id' in df_fe.columns and not is_train:\n",
    "        ids = df_fe['Id']\n",
    "        df_fe = df_fe.drop(columns=['Id'])\n",
    "    elif 'Id' in df_fe.columns and is_train:\n",
    "        df_fe = df_fe.drop(columns=['Id'])\n",
    "\n",
    "    # --- Features Numéricas ---\n",
    "    df_fe[\"Food_Drinks\"] = df_fe[\"Food\"] + df_fe[\"Drinks\"]\n",
    "    df_fe[\"Spending\"] = df_fe[\"Food\"] * df_fe[\"Drinks\"]\n",
    "    # Ratio de gasto (evitar división por cero sumando 1)\n",
    "    df_fe[\"Food_Ratio\"] = df_fe[\"Food\"] / (df_fe[\"Food_Drinks\"] + 1)\n",
    "\n",
    "    # --- Features de Ticket ---\n",
    "    # Parseo robusto\n",
    "    split_ticket = df_fe[\"TicketInfo\"].str.split(\"/\", expand=True)\n",
    "    df_fe[\"Ticket_Zone\"] = split_ticket[0]\n",
    "    df_fe[\"Ticket_Row\"] = pd.to_numeric(split_ticket[1], errors='coerce').fillna(-1)\n",
    "    \n",
    "    # Feature de \"Primera Fila\" vs \"Fondo\"\n",
    "    df_fe['Is_Front_Row'] = (df_fe['Ticket_Row'] <= 5).astype(int)\n",
    "    df_fe['Is_Back_Row'] = (df_fe['Ticket_Row'] > 20).astype(int)\n",
    "    df_fe['Is_VIP'] = df_fe['Ticket_Zone'].str.contains('VIP', case=False, na=False).astype(int)\n",
    "    \n",
    "    # --- Features de Edad ---\n",
    "    df_fe['Age'] = df_fe['Age'].fillna(df_fe['Age'].median())\n",
    "    df_fe['Is_Child'] = (df_fe['Age'] < 12).astype(int)\n",
    "    \n",
    "    # --- Features de Opinión (Meta-data) ---\n",
    "    df_fe['Opinion_Length'] = df_fe['Opinion'].str.len().fillna(0)\n",
    "    df_fe['Has_Opinion'] = df_fe['Opinion'].notna().astype(int)\n",
    "    \n",
    "    # IMPORTANTE: Generamos la columna de texto limpio para el TF-IDF\n",
    "    df_fe['Opinion_Clean'] = clean_text(df_fe['Opinion'])\n",
    "    \n",
    "    # Convertir booleanos a int\n",
    "    for col in df_fe.columns:\n",
    "        if df_fe[col].dtype == 'bool':\n",
    "            df_fe[col] = df_fe[col].astype(int)\n",
    "            \n",
    "    return df_fe, ids\n",
    "\n",
    "# Preparar datos\n",
    "print(\"Preparando datos...\")\n",
    "# Asumiendo que 'X' y 'y' ya están cargados en tu entorno (de tu código original)\n",
    "# Si no, cárgalos con: df_train = pd.read_csv(\"train.csv\"); X = df_train.drop(\"FreePass\", axis=1); y = df_train[\"FreePass\"]\n",
    "X_fe, _ = prepare_data(X, is_train=True)\n",
    "X_test_fe, test_ids = prepare_data(df_test, is_train=False)\n",
    "\n",
    "# Asegurar columnas consistentes (rellenar faltantes en test con 0)\n",
    "for col in set(X_fe.columns) - set(X_test_fe.columns):\n",
    "    X_test_fe[col] = 0\n",
    "X_test_fe = X_test_fe[X_fe.columns]\n",
    "\n",
    "# =====================\n",
    "# 3. DEFINICIÓN DE COLUMNAS PARA PIPELINE\n",
    "# =====================\n",
    "text_col = 'Opinion_Clean'\n",
    "# Excluimos columnas que no van al modelo numérico/categórico directo\n",
    "exclude_cols = [text_col, 'Opinion', 'TicketInfo'] \n",
    "\n",
    "num_cols = [c for c in X_fe.select_dtypes(include=['float64','int64']).columns if c not in exclude_cols]\n",
    "cat_cols = [c for c in X_fe.select_dtypes(include=['object','category']).columns if c not in exclude_cols]\n",
    "\n",
    "print(f\"Num cols: {len(num_cols)}, Cat cols: {len(cat_cols)}, Text col: 1\")\n",
    "\n",
    "# =====================\n",
    "# 4. PIPELINE AVANZADO CON NLP\n",
    "# =====================\n",
    "\n",
    "# Pipeline Numérico\n",
    "num_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline Categórico\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "# Pipeline de Texto (TF-IDF)\n",
    "# max_features=100: Toma las 100 palabras más relevantes\n",
    "text_pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=100, stop_words='english', ngram_range=(1,1)))\n",
    "])\n",
    "\n",
    "# ColumnTransformer GLOBAL\n",
    "# sparse_threshold=0 ES LA CLAVE PARA CORREGIR TU ERROR\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols),\n",
    "    (\"cat\", cat_pipe, cat_cols),\n",
    "    (\"text\", text_pipe, text_col) \n",
    "], sparse_threshold=0) \n",
    "\n",
    "# =====================\n",
    "# 5. MODELO HÍBRIDO (ENSEMBLE)\n",
    "# =====================\n",
    "\n",
    "# Modelo 1: Gradient Boosting (Potente)\n",
    "gb_model = HistGradientBoostingClassifier(\n",
    "    max_iter=500,\n",
    "    learning_rate=0.04,\n",
    "    max_depth=10,\n",
    "    l2_regularization=0.5,\n",
    "    early_stopping=True,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Modelo 2: Random Forest (Robusto)\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=12,\n",
    "    min_samples_split=10,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Voting Classifier (Mezcla ambos)\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('gb', gb_model),\n",
    "        ('rf', rf_model)\n",
    "    ],\n",
    "    voting='soft', # Promedia probabilidades\n",
    "    weights=[0.6, 0.4]\n",
    ")\n",
    "\n",
    "final_pipeline = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", ensemble)\n",
    "])\n",
    "\n",
    "# =====================\n",
    "# 6. VALIDACIÓN CRUZADA ROBUSTA (Stratified K-Fold)\n",
    "# =====================\n",
    "print(\"Entrenando con Cross-Validation y optimizando Threshold...\")\n",
    "\n",
    "# Generar predicciones Out-Of-Fold (OOF)\n",
    "# Esto simula predicciones en test data usando todo el train set mediante k-fold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_oof_probs = cross_val_predict(final_pipeline, X_fe, y, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "# Buscar el mejor punto de corte (Threshold)\n",
    "thresholds = np.linspace(0.2, 0.8, 100)\n",
    "best_t, best_f1 = 0.5, 0\n",
    "\n",
    "for t in thresholds:\n",
    "    preds = (y_oof_probs > t).astype(int)\n",
    "    score = f1_score(y, preds)\n",
    "    if score > best_f1:\n",
    "        best_f1 = score\n",
    "        best_t = t\n",
    "\n",
    "print(f\"CV F1 Score estimado: {best_f1:.4f} | Mejor Threshold: {best_t:.3f}\")\n",
    "\n",
    "# =====================\n",
    "# 7. ENTRENAMIENTO FINAL Y SUBMISSION\n",
    "# =====================\n",
    "print(\"Re-entrenando modelo final con TODO el dataset...\")\n",
    "final_pipeline.fit(X_fe, y)\n",
    "\n",
    "# Predicción en Test\n",
    "probs_test = final_pipeline.predict_proba(X_test_fe)[:, 1]\n",
    "preds_test = (probs_test > best_t).astype(bool) # Usamos el threshold optimizado\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": test_ids,\n",
    "    \"FreePass\": preds_test\n",
    "})\n",
    "\n",
    "filename = \"submission_nlp_ensemble_fixed.csv\"\n",
    "submission.to_csv(filename, index=False)\n",
    "print(f\"¡Hecho! Archivo guardado: {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
